# 面试总结

### http方法的安全/幂等

1. 方法本身：GET不发送请求体，所以相对POST是安全的
   1. GET方法并不是强制不能带包体，只是部分包/库进行了限制
2. 对服务来说：
   1. ![HTTP方法安全性](H:\go\markdown\面试\img\HTTP方法安全性.jpg)
   2. 多次请求，对服务器来说没有状态改变，则该方法是安全的。
      1. 就是所谓的请求是幂等的，幂等指的是结果而不是请求到的资源本身
      2. 在编程中，同一方法多次运算，得到的结果是一致的，就可以说方法是幂等的，也就是安全的

### utxo 和balance 

1. utxo

   > Unspent  Transcation Output  ，即未花费的交易输出，其实没有什么比特币，只有utxo，比特币钱包就是代为统计这些utxo
   >
   > transcation 交易在比特币系统中简称tx，交易就是utxo的销毁和生成的过程，生成的时候提供锁定脚本，解锁时提供解锁脚本
   >
   > utxo模型允许并行交易的存在，系统追踪的是utxo的销毁和生成，而不是谁将比特币转给谁
   >
   > 核心思想：写入的数据不可变，通过哈希指针连接不同交易的输入输出，保证交易的合法性
   >
   > 好处：
   >
   > 1. 可扩展性：可以支持同时处理多个UTXO，所以可以实现并行事务并鼓励可伸缩创新
   > 2. 隐私，为每笔交易创建新的地址，可以提供高级别的安全性

2. 账户余额模型（balance）

   > 每个账户的余额保持为全球状态，检查账户余额确保大于或等于支出的金额
   >
   > 使用交易中的nonce（每次支付自增1） 来防止双重支付
   >
   > 抽象看eth，只有三种对象：账户，交易，区块
   >
   > 优势：
   >
   > 1. 实现简单
   > 2. 验证时效率较高
   >    1. 交易的输入输出都是地址，存储空间也小
   > 3. 创建交易不需要对UTXO签名，所以可以从任意一个节点同步区块，便于编写轻量级客户端

### solidity safemath

> 防止计算溢出造成代币超发，
>
> https://github.com/OpenZeppelin/openzeppelin-solidity/blob/master/contracts/math/SafeMath.sol

### mysql

1. 隔离级别

   > | 事务隔离级别                 | 脏读 | 不可重复读 | 幻读 |
   > | ---------------------------- | ---- | ---------- | ---- |
   > | 读未提交（read-uncommitted） | 是   | 是         | 是   |
   > | 读已提交（read-committed）   | 否   | 是         | 是   |
   > | 可重复读（repeatable-read）  | 否   | 否         | 是   |
   > | 串行化（serializable）       | 否   | 否         | 否   |
   >
   > innodb  rr级别的隔离主要通过mvcc实现，可以重复读，但无法防止幻读

2. MVCC(多版本并发控制)

   > 连接数据库的操作，在某个瞬间看到的都是数据库的一个快照
   >
   > innodb的行锁定依靠该机制实现
   >
   > innodb的MVCC不同于经典MVCC，在进行写操作的时候，使用了排他锁
   >
   >  	1. 经典MVCC不使用锁，完全依赖多版本记录进行控制
   >  	2. 当对同一记录多次操作后，对不是最近一次操作实施回滚操作，传统MVCC会覆盖该操作后面的动作，造成数据污染
   >
   > innodb中MVCC通过undo.log实现，
   >
   > innodb在存放数据时，会存放几个隐藏字段：
   >
   >  	1. DATA_ROLL_PTR
   >       ​	1. 指向回滚用的版本 --- undolog  日志记录
   >  	2. DATA_TRX_ID
   >       ​	1. 最近更新数据的事务ID
   >  	3. DB_ROW_ID
   >  	4. DELETE_BIT
   >       ​	1. 标识被删除
   >
   > 执行过程：
   >
   > Begin --> 使用排他锁 --> 写入redolog   -->   记录undolog  -->  修改内存中数据，事务编号等   --> commit阶段刷入binlog日志（prepare阶段），刷入事务日志

3. redolog

   > redolog日志是物理日志，所以对同一条数据的操作是幂等的
   >
   > redolog  是物理日志，所以恢复速度远远快于binlog日志
   >
   > 三个位置：
   >
   > 1. buffer redo(用户空间)
   > 2. os redo(内核空间)
   > 3. file  redo(文件系统)
   >    1. innodb_flush_log_at_trx_commit = 1 ,表示每次commit提交，就将redolog刷入磁盘
   >    2. innodb_flush_log_at_trx_commit = 0，每秒将buffer写入os redo，然后刷入磁盘，每次commit不会写入os redo
   >    3. innodb_flush_log_at_trx_commit = 2，每次提交都写入os redo，每秒调用fsync写入磁盘
   >    4. 生产中常设为2， 这样兼容了安全性和效率（buffer--> os  内存之间转移，很快）

4. undolog

   > 两个功能：
   >
   > 1. 提供回滚
   >    1. 理解为，事务中的每次操作，都会在undolog中记录一条用于回滚的相反操作（update主键时特殊，这时是相当于两条操作，先删除，再插入）
   > 2. 提供MVCC机制
   >    1. 当读取记录时，记录被其他事务读锁定，通过undolog中记录可获取锁定前的数据，从而提供该行版本信息
   >    2. 事务提交时，undolog并不会被立即删除，因为有可能被其他事务锁定，只要该行版本还被引用，就不能删除
   >
   > 属于逻辑日志：
   >
   > 默认存放在ibdata1中

5. binlog

   > binlog日志是上层系统的日志，redolog undolog都是innodb引擎生成的日志

6. commit

   >  group commit
   >
   > 一样先刷二进制，再刷事务日志
   >
   > commit阶段开始commit  prepare时，为了保证二进制日志和事务日志的顺序性，使用一个独占锁进行控制，破坏了group commit
   >
   > mysql5.6以后，改进为通过一个队列存放事务，commit阶段保证顺序性

7. 双主架构

   > 通过配置主主赋值 + keepalived 实现
   >
   > 关键点：
   >
   > 1. 两个节点的auto_increment_increment（自增步长）和auto_increment_offset（自增起始值）设为不同值
   > 2. keepalived 设定为抢占模式，防止脑裂后网络恢复后出现VIP不释放的状况
   > 3. 为防止从服务器同步延迟，可以使用mysql5.7以上版本，提供了多线程复制

8. 分库分表

   > 业务分库
   >
   > 1. 按照业务对表进行拆分，可以存放于不同实例，回导致join无法使用
   >
   > 垂直切分
   >
   >  	1. 按字段条件切分
   >  	2. 按范围切分
   >  	3. 取模切分
   >       ​	1. 这种切分扩容会很麻烦，推荐倍数扩容
   >
   > 数据库分区
   >
   >  	1. 数据库自身的特性
   >
   > 带来的问题：
   >
   > 1. 分库分表时的复杂性
   > 2. 数据库架构变复杂
   > 3. 跨节点无法join
   > 4. 扩容/数据迁移艰难
   >    1. 推荐业务双写迁移
   >       1. 上线双写业务  -->   迁移就数据  -->  验证迁移数据 -->  切换读路由  --> 下线双写业务
   >       2. 要明确什么样角色的什么人在什么时候到什么系统看到什么样的数据，才能确认迁移成功。
   >    2. https://blog.csdn.net/qq_16605855/article/details/80966363
   >    3. 停机迁移
   >    4. 迁移后的数据清洗：
   >       1. 能全量对比，就全量对比，不能就抽样对比
   >       2. 通过服务日志，判定数据不一致的原因
   >       3. 可能的原因：
   >          1. 更新/删除数据最容易产生问题，迁移某条数据时，正在进行操作，迁移系统已获取数据，两方同时进行了修改，然后迁移系统又对数据进行了修改，造成数据不一致，通过binlog日志timestamp进行区分

9. 宕机回滚

   >```
   >1. 扫描日志，找出所有已经START,还没有COMMIT的事务。
   >2. 针对所有未COMMIT的日志，根据redo log来进行回滚。 
   >```

10. checkpoint

    > 将内存中的脏数据/脏日志刷入磁盘

11. innodb回滚操作

    > 不管开机前是否是正常关机，开机后，系统都会进行恢复行为
    >
    > checkpoint表示之前的操作都已经刷入磁盘，所以仅需从checkpoint开始的位置开始恢复
    >
    > 如果宕机时，正在checkpoint，刷盘数据页中记录的LSN大于日志页中的LSN，不需要重做

12. sql

    > 1. select singer_id, group_concat(song_id) from song_singer group  by singer_id;
    >
    > 2. select singer.id, singer.singer_name, sum(song_stats.listen_count) from song_stats inner join song_singer on song_stats.song_id = song_singer.song_id inner join singer on song_singer.singer_id = singer.id inner join song on song_stats.song_id = song.id group by song_singer.singer_id having sum(song_stats.listen_count) > 100;
    > 3.  select uid from   (select uid,sum(score),date from test1 group by uid,date having sum(score) > 10 ) as subtable group by uid having count(uid) > 0
    > 4. select date_add("2018-11-11", interval 1 day)
    > 5. select date_format(now(), '%m-%d-%Y')     date 五种类型：date  time  datetime timestamp  year
    > 6. VARCHAR值保存时只保存需要的字符数，另加一个字节来记录长度(如果列声明的长度超过255，则 使用两个字节)

### 缓存策略

> 不选择更新缓存，而是直接删除，是由于，更新操作可能会伴随着附加的操作，或者写多读少场景，每次修改都更新缓存资源浪费
>
> 为缓存设置失效时间，是保证缓存最终一致的保障
>
> 使用延迟双删除策略，保证更新数据后，缓存一定会失效
>
>  1. 使用一个单独的异步操作进行，防止阻塞造成通途量下降
>

1. 先更新数据库，再删除缓存

   > 可能造成不一致的原因，一般都是因为高并发产生
   >
   > 1. 发生的可能性不大，因为这种情况有个前提，某次写操作快于读操作

2. 先删除缓存，再更新数据库

   > 可能不一致原因：
   >
   > 1. 高并发
   > 2. 读写分离   ---> 还没同步过去，已经发生了读操作

### redis

1. string

2. list

3. set

4. zset

   > 底层通过跳跃表实现快速索引
   >
   > 底层skiplist 与dict的结合
   >
   > 利用zset 可以实现快速排序（score）

5. hash

   > 底层使用双向链表实现，所以写入快速，读取效率低

### 大流量冲击

1. 服务降级	

   > 服务降级，一般从最外层服务开始

2. 服务下线

3. 弹性扩容

4. 服务限流

### 一致性hash（nginx底层调度算法）

1. 概念

   > 单纯使用hash%N (哈希取余)，扩容时会有大量的key失效迁移，
   >
   > 按权重，生成不同的虚拟节点（每个虚拟节点hash生成时加入了上一个节点的hash，确保不一样），有了全部的hash节点后，排序形成环

2. go实现

   > golang  sort接口的search方法实现获取最合适的元素

### slice

> slice是数组的一个视图
>
> 保存有一个头指针，len, cap

### rabbitMq  Kafka

1. rabbitMq

   > 1. 一种典型的p2p(point to point)模式的消息队列
   >    1. 可以通过设置交换器类型实现发布订阅模式达到广播的效果
   > 2. 支持消息堆积
   >    1. 典型的内存式消息堆积，但并不是绝对的，在某些条件触发后，会有换页动作将内存中的消息换页到磁盘（会影响吞吐量），或者直接使用惰性队列将消息直接持久化到磁盘
   >    2. 内存堆积达到阈值，会影响性能
   > 3. 支持事务性消息
   >    1. 指生产者发生消息的事务，要么发送成功，要么发送失败
   > 4. 消息消费使用 推模式+拉模式
   > 5. 支持优先级队列
   >    1. 建议优先级大小设置在0-10之间
   > 6. 支持延迟队列
   > 7. 支持死信队列
   > 8. 支持重试队列
   >    1. 通过封装延迟队列可较轻松实现
   > 9. 不支持消息回溯
   >    1. 消息一旦被确认消费，就会被标记删除
   > 10. 对广播消费支持相对弱
   > 11. 支持持久化
   > 12. 支持消息追踪，rabbitma_tracing插件会大幅影响性能，不建议使用
   > 13. 支持多租户模式

   > 1. AMQP，即Advanced Message Queuing Protocol，高级消息队列协议，是应用层协议的一个开放标准，使用github.com/streadway/amqp
   >    1. ConnectionFactory、Connection、Channel
   > 2. 为防止消息丢失，采用消息回执方式（没有超时时间机制）
   >    1. 相当于内部维持两个队列，一个存放未发送的消息队列，一个存放未回执已发送的队列，如果未收到回执且comsumer断开连接，则将消息放回未发送的消息队列
   > 3. Message durability
   >    1. 三部分：exchange/queue/message  缺一项都不能保证故障恢复数据
   >    2. 用于防止服务器宕机重启之后数据的丢失
   >    3. 刷盘机制
   >       1. buffer默认1m，满了就刷盘
   >       2. 25ms超时刷盘
   >       3. 没有后续写入请求，会触发erlang的一个timeout机制触发刷盘
   >    4. 删除策略
   >       1. 所有文件中垃圾信息（已经被消费的消息）超过阈值（GARBAGE_FRACTION=0.5）时，触发文件合并操作（起码有三个文件存在），提高磁盘利用率
   > 4. producter
   >    1. 消息发送给exchange，会指定一个routing key ,结合exchange type 和binding key结合进行路由
   >    2. routing key 限长  255 bytes
   > 5. exchange
   >    1. types（固定配置）: 
   >       1. topic              * 表示一个单词，#表示多个单词可以为零，多个单词之间使用“.”连接
   >       2. fanout      广播给所有绑定的queue
   >       3. direact      完全匹配给对应的queue    routing Key == binding  key
   >       4. headers      不依赖 routing key  和 binding key匹配规则来进行路由，绑定exchange/queue时会指定一个键值对，将消息放松给exchange时，RabbitMq从消息中获取键值对，进行完全匹配路由
   >    2. binding key （绑定exchange  queue时指定）
   >       1. 作用需要依赖于exchange type，如fanout /headers就会忽略binding可以进行路由
   > 6. queue
   >    1. 多个consumer订阅同一个queue消息会被平均分摊，可以设置prefetchCount限制Queue每次发送给每个Comsumer的消息数（为了匹配不同comsumer的消费能力）
   > 7. consumer

2. Kafka

   > 1. 典型的订阅发布模式
   >    1. 可以将Consumer  group看作队列
   > 2. 拥有消息回溯功能，对广播消费支持力度更强
   >    1. 通过offset进行回溯，重新消费已经消费过的消息
   > 3. 典型的磁盘式堆积，所有消息都存在磁盘中
   >    1. 容量会比内存的容量大得多
   >    2. 为消息中间件提供了冗余存储的功能，甚至可以直接作为存储服务器
   > 4. 支持事务性消息
   > 5. 消息消费使用拉模式
   > 6. 不支持多租户模式/优先级队列/延迟队列/重试队列
   > 7. 对广播消费支持完善

   > 1. topic
   >    1. 逻辑概念
   > 2. partition
   >    1. 物理概念，在log.dirs指定的目录存放
   >    2. 目录名：topic名+自增序号

3. etcd
   1. etcd作为最近很火的一个高可用性?键值对服务发现系统被Kubernetes等系统广泛使用

   2. 他相比与zookeeper来说更加简单

   3. 在Raft中，任何时候一个服务器可以扮演下面角色之一：

      Leader: 处理所有客户端交互，日志复制等，一般一次只有一个Leader.

      Follower: 类似选民，完全被动

      Candidate候选人: 类似Proposer律师，可以被选为一个新的领导人。

   4. leader选举阶段

   5. 消息同步阶段

   6. Leader要求Followe遵从他的指令，都将这个新的日志内容追加到他们各自日志中：

   7. 大多数follower服务器将日志写入磁盘文件后，确认追加成功，发出Commited Ok:

   8. 在下一个心跳heartbeat中，Leader会通知所有Follwer更新commited 项目。对于每个新的日志记录，重复上述过程。

4. zookeeper：
   1. zookeeper是基于paxos的简化版zab

   2. ZAB协议中存在着三种状态，每个节点都属于以下三种中的一种：

      Looking ：系统刚启动时或者Leader崩溃后正处于选举状态

      Following ：Follower节点所处的状态，Follower与Leader处于数据同步阶段；

      Leading ：Leader所处状态，当前集群中有一个Leader为主进程；

   3. 在开始时，所有的节点都是looking状态并且每个节点都希望自己能成为leader节点，所有每个节点都会向集群中发送一个提案内容是选取自己作为leader节点，提案编号是ZXID(ZAB协议中使用ZXID作为事务编号，ZXID为64位数字，低32位为一个递增的计数器，每一个客户端的一个事务请求时Leader产生新的事务后该计数器都会加1，高32位为Leader周期epoch编号，当新选举出一个Leader节点时Leader会取出本地日志中最大事务Proposal的ZXID解析出对应的epoch把该值加1作为新的epoch，将低32位从0开始生成新的ZXID；ZAB使用epoch来区分不同的Leader周期),如果得到的提案的zxid比自己的大则说明发出这个题案的节点数据更新，则进行同意的投票，否则继续投自己，先得到多数的同意的节点当选为leader

   4. 现在leader就可以进行管理了，zookeeper也是两段提交的实现，客户端提交事务请求时Leader节点为每一个请求生成一个事务Proposal，将其发送给集群中所有的Follower节点，收到过半Follower的反馈后开始对事务进行提交，ZAB协议使用了原子广播协议